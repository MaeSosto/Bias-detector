{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install transformers\n",
    "#!pip install pandas\n",
    "#%pip install torch\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.13.0\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Set the device      \n",
    "device = \"mps\" if torch.backends.mps.is_available() else torch.device(\"cuda\") if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1: https://huggingface.co/roberta-base\n",
    "Model 2: https://huggingface.co/vinai/bertweet-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntrasentenceEvaluator():\n",
    "    \n",
    "    def __init__(self, data, choices, model):\n",
    "        self.data = data\n",
    "        self.choices = choices\n",
    "        self.model = model\n",
    "        self.process_sentences()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        self.sent_encodings, self.word_encodings, self.mask_idxs = self.make_encodings() #store the encodings\n",
    "        self.model = AutoModelForMaskedLM.from_pretrained(model)\n",
    "    \n",
    "    #Function to make predictions and calculate how often the biased words are chosen\n",
    "    def run_model_and_evaluate(self):\n",
    "        output = self.make_predictions()\n",
    "        # self.bias = self.get_bias(output) #how often do we get the bias\n",
    "        self.get_bias(output)\n",
    "\n",
    "    #Insert the candidates words inside the sentences\n",
    "    def process_sentences(self,s='______'):\n",
    "        candidate_sentence = []\n",
    "        for index,row in self.data.iterrows():\n",
    "            candidate_sentence.append([re.sub(s,row.loc[c], row.loc['sentence']) for c in self.choices.keys()]) #replace s with candidate words\n",
    "        self.data.loc[:,'candidate_sentence'] = candidate_sentence\n",
    "    \n",
    "    #find the mask indices for the encoded sentence.\n",
    "    def get_sublist_idxs_in_list(self, word, sentence):\n",
    "        possibles = np.where(sentence==word[0])[0] #where my sentence is equal to my word\n",
    "        for p in possibles: #loop over the possibilities\n",
    "            check = sentence[p:p+len(word)] #if the word is based on two tokens then I'm gonna look for them \n",
    "            if np.all(check == word):\n",
    "                return list(range(p,(p+len(word)))) #return back the positions of the tokens\n",
    "    \n",
    "    #Function to make encodings: We go over all candidate sentences and encode the words and look for the indices of the placed words.\n",
    "    def make_encodings(self): \n",
    "        sent_encoding = [] #tokenized sentenced\n",
    "        word_encoding = [] #tokenized words\n",
    "        mask_idxs = [] #the indexes where the tokens of the choices are, i.e. where the <mask> should be into the candidates sentences\n",
    "        for index,row in self.data.iterrows():\n",
    "            _sent_encoding,_word_encoding,_mask_idxs=[],[],[] #sublists, we have 3 for each sentences\n",
    "            for i,(word,sentence) in enumerate(zip(row[self.choices.keys()],row.loc['candidate_sentence'])): #for each sentences we creted in the previous function\n",
    "                encoded_word = self.tokenizer.encode(str(\" \"+ word),add_special_tokens=False) #Roberta is greedy, needs space in front of a word to realize that it is a new word and not part of the one in front\n",
    "                encoded_sent = self.tokenizer.encode_plus(sentence, add_special_tokens = True, return_tensors = 'pt', padding='max_length', max_length=128, return_attention_mask=True)\n",
    "                tokens_to_mask_idx = self.get_sublist_idxs_in_list(np.array(encoded_word),np.array(encoded_sent['input_ids'][0])) #go through encoded_sent and find position of encoded_word\n",
    "                encoded_sent['input_ids'][0][tokens_to_mask_idx] = self.tokenizer.mask_token_id #replace tokens with mask_token, since now we are working with tokens\n",
    "                _sent_encoding.append(encoded_sent)\n",
    "                _word_encoding.append(encoded_word)\n",
    "                _mask_idxs.append(tokens_to_mask_idx)\n",
    "            sent_encoding.append(_sent_encoding)\n",
    "            word_encoding.append(_word_encoding)\n",
    "            mask_idxs.append(_mask_idxs)\n",
    "        return sent_encoding , word_encoding , mask_idxs\n",
    "    \n",
    "    #Function to make predictions:\n",
    "    # We go over all sentences with help of the made encoding and see which placed words in the candidate sentences return the highest probability of being chosen.\n",
    "    # We also see which words the mask filler, i.e. our model, would choose itself for the masked token.\n",
    "    def make_predictions(self):\n",
    "        output = [] #we want what option with highest probability has been chosen\n",
    "        for q_idx, (w, s, m) in enumerate(zip(self.word_encodings, self.sent_encodings, self.mask_idxs)):\n",
    "            predictions =[]\n",
    "            candidate_input_ids = torch.stack([inp_ids['input_ids'].squeeze(0) for inp_ids in s]) #we create batch so instead do precition one by one, the model can predict the whole batch, we create a batch for each sentence \n",
    "            candidate_attention_masks = torch.stack([am['attention_mask'].squeeze(0) for am in s])\n",
    "            candidate_logits = self.model(candidate_input_ids, attention_mask=candidate_attention_masks).logits #where logits is  raw output of the model -> prediction\n",
    "            # -> output shape: 3 * num of tokens*vocab size, e.g. a prediction over the vocabulary for each token in each candidate question\n",
    "            # -> probability distribution over the whole vocab size\n",
    "            for idx, (token, mask_idxs) in enumerate(zip(w, m)): #for each of the 3 candidate sentences, we need to pick out the token that we masked in the sentence\n",
    "                mask_token_logits = candidate_logits[idx, mask_idxs, token] # here we want to find the raw prediction for the candidate word\n",
    "                candidate_score = float(torch.mean(mask_token_logits)) #if we have more than one mask this is our \"pseudo accuracy\"\n",
    "                predictions.append(candidate_score)\n",
    "            #print(f\"iprediction: {q_idx}, values: {predictions}\")\n",
    "            output.append(np.argmax(predictions) + 1) #start the keys for the choices at 1 as well > returns the choice that is chosen as an answer, we don't need that\n",
    "        #print(output)\n",
    "        return output\n",
    "    \n",
    "    #Function to see how often the biased words were chosen.\n",
    "    def get_bias(self,predictions):\n",
    "        biased, unbiased, unrelated = 0, 0, 0\n",
    "        for pred in predictions:\n",
    "            if pred == 1:\n",
    "                biased +=1\n",
    "            if pred == 2:\n",
    "                unbiased += 1\n",
    "            if pred == 3:\n",
    "                unrelated += 1\n",
    "        print(f\"biased: {biased}\")\n",
    "        print(f\"unbiased: {unbiased}\")\n",
    "        print(f\"unrelated: {unrelated}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biased: 14\n",
      "unbiased: 5\n",
      "unrelated: 0\n"
     ]
    }
   ],
   "source": [
    "choices = {'bias':1, 'unbiased':2,'unrelated':3} \n",
    "file_path = 'testing_data_intra.csv' #\"drive/MyDrive/Final_templates.csv\"#\"drive/MyDrive/New_templates.csv\"\n",
    "model_name = 'distilroberta-base'\n",
    "templates = pd.read_csv(file_path, sep=\";\")\n",
    "evaluator = IntrasentenceEvaluator(templates.copy(), choices, model_name)\n",
    "evaluator.run_model_and_evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  5 2022, 01:52:34) \n[Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c431e0593e1ee77f96bd6f746b63b987ead1ae8f9402a6438f4058638187923"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
