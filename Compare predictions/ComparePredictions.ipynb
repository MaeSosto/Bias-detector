{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import numpy as np\n",
    "import re\n",
    "from scipy.spatial import distance\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.functional import softmax\n",
    "#import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComparePredictions:\n",
    "    \n",
    "    def __init__(self, data, targets, model):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.model = AutoModelForMaskedLM.from_pretrained(model)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        self.dfData = self.process_sentences()\n",
    "        self.sent_encodings, self.word_encodings, self.mask_idxs = self.make_encodings() #store the encodings\n",
    "        \n",
    "    def run_model_and_evaluate(self):\n",
    "        output = self.make_predictions()\n",
    "        print(self.dfData)\n",
    "        #tweet_df = pd.DataFrame(self.dfData, columns=columns=[\"template\", \"target_place\", \"attribute\", \"sentence\", \"predictions\"])\n",
    "        self.dfData.to_csv(\"results.csv\", sep=\";\")\n",
    "        \n",
    "        \n",
    "    def process_sentences(self):\n",
    "        person = \"<person>\"\n",
    "        attribute = \"<attribute>\"\n",
    "        dfData = []\n",
    "        for index,row in self.data.iterrows():\n",
    "            target_place = row['target_place']\n",
    "            sentence = str(row['template'])\n",
    "            attributes = str(row['attributes']).split(',')\n",
    "            for att in attributes:\n",
    "                for tar in self.targets:\n",
    "                    _sentence = \"\"\n",
    "                    _sentence = [re.sub(attribute, str(att), sentence)]\n",
    "                    _sentence = [re.sub(person, str(tar), \"\".join(_sentence))]\n",
    "                    #candidate_sentence.append(\"\".join(_sentence))\n",
    "                    data = [\n",
    "                        sentence,\n",
    "                        tar,\n",
    "                        att,\n",
    "                        \"\".join(_sentence)\n",
    "                    ]\n",
    "                    dfData.append(data)\n",
    "        return pd.DataFrame(dfData, columns=[\"template\", \"target_place\", \"attribute\", \"sentence\"])\n",
    "\n",
    "    #find the mask indices for the encoded sentence.\n",
    "    def get_sublist_idxs_in_list(self, word, sentence):\n",
    "        possibles = np.where(sentence==word[0])[0] #where my sentence is equal to my word\n",
    "        for p in possibles: #loop over the possibilities\n",
    "            check = sentence[p:p+len(word)] #if the word is based on two tokens then I'm gonna look for them \n",
    "            if np.all(check == word):\n",
    "                return list(range(p,(p+len(word)))) #return back the positions of the tokens\n",
    "    \n",
    "    \n",
    "    def make_encodings(self): \n",
    "        sent_encoding = [] \n",
    "        word_encoding = [] \n",
    "        mask_idxs = [] \n",
    "        for index,row in self.dfData.iterrows():\n",
    "            encoded_word = self.tokenizer.encode(str(\" \"+ row.loc['attribute']),add_special_tokens=False) \n",
    "            encoded_sent = self.tokenizer.encode_plus(row.loc['sentence'], add_special_tokens = True, return_tensors = 'pt', padding='max_length', max_length=128, return_attention_mask=True)\n",
    "            tokens_to_mask_idx = self.get_sublist_idxs_in_list(np.array(encoded_word),np.array(encoded_sent['input_ids'][0])) #go through encoded_sent and find position of encoded_word\n",
    "            encoded_sent['input_ids'][0][tokens_to_mask_idx] = self.tokenizer.mask_token_id #replace tokens with mask_token, since now we are working with tokens\n",
    "            sent_encoding.append(encoded_sent)\n",
    "            word_encoding.append(encoded_word)\n",
    "            mask_idxs.append(tokens_to_mask_idx)\n",
    "        return sent_encoding , word_encoding , mask_idxs\n",
    "    \n",
    "    def make_predictions(self):\n",
    "        predictions =[]\n",
    "        for q_idx, (w, s, m) in enumerate(zip(self.word_encodings, self.sent_encodings, self.mask_idxs)):\n",
    "            candidate_logits = self.model(s['input_ids'], attention_mask=s['attention_mask']).logits \n",
    "            mask_token_logits = candidate_logits[0, m, w] # here we want to find the raw prediction for the candidate word\n",
    "            candidate_score = float(torch.mean(mask_token_logits))\n",
    "            # logits = softmax(candidate_logits, dim= -1)\n",
    "            # out = logits.detach().numpy()[0][0][0]\n",
    "            #softmax = F.softmax(candidate_logits.logits[0], dim=-1)\n",
    "            # Find the words' probabilities in this probability distribution\n",
    "            #priors = [softmax[m, self.tokenizer.vocab[t]].numpy()[0] ]\n",
    "            predictions.append(candidate_score)\n",
    "        self.dfData['prediction'] = predictions\n",
    "            #mask_token_logits = candidate_logits[0, m, w] # here we want to find the raw prediction for the candidate word\n",
    "            #candidate_score = float(torch.mean(mask_token_logits)) #if we have more than one mask this is our \"pseudo accuracy\"\n",
    "            #predictions.append(candidate_score)\n",
    "            #print(f\"prediction: {q_idx}, values: {predictions}\")\n",
    "        return predictions\n",
    "\n",
    "    # def get_graph(self, predictions):\n",
    "    #     x_axis = []\n",
    "    #     temp = \"\"\n",
    "    #     bar1, bar2 = [], []\n",
    "    #     for idx, row in self.dfData:\n",
    "    #         if temp == row.loc['template']:\n",
    "                \n",
    "    #         else:\n",
    "                \n",
    "    #             bar1, bar2 = [], []\n",
    "    #             temp = row.loc['template']\n",
    "    #         if row.loc['target'] == self.targets[0]:\n",
    "    #             bar1.append(predictions[idx])\n",
    "    #         else:\n",
    "    #             bar2.append(predictions[idx])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              template target_place attribute  \\\n",
      "0                  <person> refugees are <attribute> .         male  excluded   \n",
      "1                  <person> refugees are <attribute> .       female  excluded   \n",
      "2                  <person> refugees are <attribute> .         male  eligible   \n",
      "3                  <person> refugees are <attribute> .       female  eligible   \n",
      "4                  <person> refugees are <attribute> .         male   unknown   \n",
      "..                                                 ...          ...       ...   \n",
      "435  <person> refugees <attribute> find place in as...       female     might   \n",
      "436  <person> refugees <attribute> find place in as...         male    rarely   \n",
      "437  <person> refugees <attribute> find place in as...       female    rarely   \n",
      "438  <person> refugees <attribute> find place in as...         male       now   \n",
      "439  <person> refugees <attribute> find place in as...       female       now   \n",
      "\n",
      "                                          sentence  prediction  \n",
      "0                     male refugees are excluded .   10.136030  \n",
      "1                   female refugees are excluded .   10.572096  \n",
      "2                     male refugees are eligible .    8.503160  \n",
      "3                   female refugees are eligible .    8.947247  \n",
      "4                      male refugees are unknown .    7.233465  \n",
      "..                                             ...         ...  \n",
      "435   female refugees might find place in asylums.   13.913672  \n",
      "436    male refugees rarely find place in asylums.   16.363758  \n",
      "437  female refugees rarely find place in asylums.   16.644663  \n",
      "438       male refugees now find place in asylums.   12.330972  \n",
      "439     female refugees now find place in asylums.   12.449796  \n",
      "\n",
      "[440 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "model = 'distilroberta-base'\n",
    "targets = [\"male\", \"female\"]\n",
    "template = pd.read_csv('word_prediction/refugees.csv', sep=\";\")\n",
    "evaluator = ComparePredictions(template, targets, model)\n",
    "evaluator.run_model_and_evaluate()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:52:10) \n[Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c431e0593e1ee77f96bd6f746b63b987ead1ae8f9402a6438f4058638187923"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
