{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/mae/opt/miniconda3/lib/python3.9/site-packages (1.13.0)\n",
      "Requirement already satisfied: typing_extensions in /Users/mae/opt/miniconda3/lib/python3.9/site-packages (from torch) (4.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#!pip install transformers\n",
    "#!pip install pandas\n",
    "%pip install torch\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1: https://huggingface.co/roberta-base\n",
    "Model 2: https://huggingface.co/vinai/bertweet-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModelEvaluator():\n",
    "    \n",
    "    def __init__(self, data, choices, model):\n",
    "        self.data = data\n",
    "        self.choices = choices\n",
    "        self.model = model\n",
    "        #print(\"ok\")\n",
    "        self.process_sentences()\n",
    "        #print(\"ok\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        self.sent_encodings, self.word_encodings, self.mask_idxs = self.make_encodings() #store the encodings\n",
    "        self.model = AutoModelForMaskedLM.from_pretrained(model)\n",
    "    \n",
    "    #Function to make predictions and calculate how often the biased words are chosen\n",
    "    def run_model_and_evaluate(self):\n",
    "        output = self.make_predictions()\n",
    "        self.bias = self.get_bias(output)#how often do we get the bias\n",
    "\n",
    "    #Insert the candidates words inside the sentences\n",
    "    def process_sentences(self,s='______'):\n",
    "        candidate_sentence = []\n",
    "        for index,row in self.data.iterrows():\n",
    "            candidate_sentence.append([re.sub(s,row.loc[c], row.loc['sentence']) for c in self.choices.keys()]) #replace s with candidate words\n",
    "        self.data.loc[:,'candidate_sentence'] = candidate_sentence\n",
    "    \n",
    "    #find the mask indices for the encoded sentence.\n",
    "    def get_sublist_idxs_in_list(self, word, sentence):\n",
    "        possibles = np.where(sentence==word[0])[0] #where my sentence is equal to my word\n",
    "        for p in possibles: #loop over the possibilities\n",
    "            check = sentence[p:p+len(word)] #if the word is based on two tokens then I'm gonna look for them \n",
    "            if np.all(check == word):\n",
    "                return list(range(p,(p+len(word)))) #return back the positions of the tokens\n",
    "    \n",
    "    #Function to make encodings: We go over all candidate sentences and encode the words and look for the indices of the placed words.\n",
    "    def make_encodings(self): \n",
    "        sent_encoding = [] #tokenized sentenced\n",
    "        word_encoding = [] #tokenized words\n",
    "        mask_idxs = [] #the indexes where the tokens of the choices are, i.e. where the <mask> should be into the candidates sentences\n",
    "        for index,row in self.data.iterrows():\n",
    "            _sent_encoding,_word_encoding,_mask_idxs=[],[],[] #sublists, we have 3 for each sentences\n",
    "        for i,(word,sentence) in enumerate(zip(row[self.choices.keys()],row.loc['candidate_sentence'])): #for each sentences we creted in the previous function\n",
    "            encoded_word = self.tokenizer.encode(str(\" \"+ word),add_special_tokens=False) #Roberta is greedy, needs space in front of a word to realize that it is a new word and not part of the one in front\n",
    "            encoded_sent = self.tokenizer.encode_plus(sentence, add_special_tokens = True, return_tensors = 'pt', \n",
    "                                                      padding='max_length', max_length=128, return_attention_mask=True)\n",
    "            tokens_to_mask_idx = self.get_sublist_idxs_in_list(np.array(encoded_word),np.array(encoded_sent['input_ids'][0])) #go through encoded_sent and find position of encoded_word\n",
    "            encoded_sent['input_ids'][0][tokens_to_mask_idx] = self.tokenizer.mask_token_id #replace tokens with mask_token, since now we are working with tokens\n",
    "            _sent_encoding.append(encoded_sent)\n",
    "            _word_encoding.append(encoded_word)\n",
    "            _mask_idxs.append(tokens_to_mask_idx)\n",
    "        sent_encoding.append(_sent_encoding)\n",
    "        word_encoding.append(_word_encoding)\n",
    "        mask_idxs.append(_mask_idxs)\n",
    "        return sent_encoding , word_encoding , mask_idxs\n",
    "    \n",
    "    #Function to make predictions:\n",
    "    # We go over all sentences with help of the made encoding and see which placed words in the candidate sentences return the highest probability of being chosen.\n",
    "    # We also see which words the mask filler, i.e. our model, would choose itself for the masked token.\n",
    "    def make_predictions(self):\n",
    "        output = [] #we want what option with highest probability has been chosen\n",
    "        preds = []\n",
    "        for q_idx, (w, s, m) in enumerate(zip(self.word_encodings, self.sent_encodings, self.mask_idxs)):\n",
    "            predictions =[]\n",
    "            candidate_input_ids = torch.stack([inp_ids['input_ids'].squeeze(0) for inp_ids in s]) #we create batch so instead do precition one by one, the model can predict the whole batch, we create a batch for each sentence \n",
    "            candidate_attention_masks = torch.stack([am['attention_mask'].squeeze(0) for am in s])\n",
    "            candidate_logits = self.model(candidate_input_ids, attention_mask=candidate_attention_masks).logits #where logits is  raw output of the model -> prediction\n",
    "            # -> output shape: 3 * num of tokens*vocab size, e.g. a prediction over the vocabulary for each token in each candidate question\n",
    "            # -> probability distribution over the whole vocab size\n",
    "            for idx, (token, mask_idxs) in enumerate(zip(w, m)): #for each of the 3 candidate sentences, we need to pick out the token that we masked in the sentence\n",
    "                mask_token_logits = candidate_logits[idx, mask_idxs, token] # here we want to find the raw prediction for the candidate word\n",
    "                candidate_score = float(torch.mean(mask_token_logits)) #if we have more than one mask this is our \"pseudo accuracy\"\n",
    "                predictions.append(candidate_score)\n",
    "            preds.append(predictions) #store predictions\n",
    "            output.append(np.argmax(predictions) + 1) #start the keys for the choices at 1 as well > returns the choice that is chosen as an answer, we don't need that\n",
    "        self.data.loc[:,'probabilities'] = preds\n",
    "        self.data.loc[:,'output'] = output\n",
    "        preds = [self.mask_filler(sent) for sent in self.data['masked_sentence']]\n",
    "        self.data.loc[:,'predictions'] = [[p['token_str'].replace(' ','') for p in pred] for pred in preds] \n",
    "        return output\n",
    "    \n",
    "    #Function to see how often the biased words were chosen.\n",
    "    def get_bias(self,predictions):\n",
    "        correct = 0\n",
    "        for pred in predictions:\n",
    "            if pred == 1 or pred == 2: #bias is always in the first column\n",
    "                correct +=1\n",
    "        return correct/len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def evaluation(model_name, file_path, choices):\n",
    "  #model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "  #create a panda dataframe of the templates\n",
    "  #templates = pd.read_csv(file_path)\n",
    "  #print('\\n Five first entries of the templates:\\n',templates.head())\n",
    "\n",
    "  #evaluator = LanguageModelEvaluator(templates.copy(), choices, model_name)\n",
    "  #evaluator.run_model_and_evaluate()\n",
    "  #return evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011042118072509766,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 898823,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ea7a81e7ebc4d40a01f56c5f565256b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008053064346313477,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 456318,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deceda374b554ab7ab5c7490ddb07e0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0042760372161865234,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 1355863,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6b1e9580e814b428ba2819350211537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005908966064453125,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 331070498,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7eed14c84b840fdba6a61055eb82f16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/331M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (1) does not match length of index (19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m templates \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(file_path, sep\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m;\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m evaluator \u001b[39m=\u001b[39m LanguageModelEvaluator(templates\u001b[39m.\u001b[39mcopy(), choices, model_name)\n\u001b[0;32m----> 6\u001b[0m evaluator\u001b[39m.\u001b[39;49mrun_model_and_evaluate()\n",
      "Cell \u001b[0;32mIn[9], line 16\u001b[0m, in \u001b[0;36mLanguageModelEvaluator.run_model_and_evaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_model_and_evaluate\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 16\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmake_predictions()\n\u001b[1;32m     17\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_bias(output)\n",
      "Cell \u001b[0;32mIn[9], line 74\u001b[0m, in \u001b[0;36mLanguageModelEvaluator.make_predictions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     72\u001b[0m     preds\u001b[39m.\u001b[39mappend(predictions) \u001b[39m#store predictions\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     output\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39margmax(predictions) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m#start the keys for the choices at 1 as well > returns the choice that is chosen as an answer, we don't need that\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mloc[:,\u001b[39m'\u001b[39;49m\u001b[39mprobabilities\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m=\u001b[39m preds\n\u001b[1;32m     75\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mloc[:,\u001b[39m'\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m output\n\u001b[1;32m     76\u001b[0m preds \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask_filler(sent) \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[\u001b[39m'\u001b[39m\u001b[39mmasked_sentence\u001b[39m\u001b[39m'\u001b[39m]]\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/pandas/core/indexing.py:716\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[1;32m    715\u001b[0m iloc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39miloc\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39miloc\n\u001b[0;32m--> 716\u001b[0m iloc\u001b[39m.\u001b[39;49m_setitem_with_indexer(indexer, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1625\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1622\u001b[0m \u001b[39m# add a new item with the dtype setup\u001b[39;00m\n\u001b[1;32m   1623\u001b[0m \u001b[39mif\u001b[39;00m com\u001b[39m.\u001b[39mis_null_slice(indexer[\u001b[39m0\u001b[39m]):\n\u001b[1;32m   1624\u001b[0m     \u001b[39m# We are setting an entire column\u001b[39;00m\n\u001b[0;32m-> 1625\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj[key] \u001b[39m=\u001b[39m value\n\u001b[1;32m   1626\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1627\u001b[0m \u001b[39melif\u001b[39;00m is_array_like(value):\n\u001b[1;32m   1628\u001b[0m     \u001b[39m# GH#42099\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/pandas/core/frame.py:3655\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3652\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   3653\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3654\u001b[0m     \u001b[39m# set column\u001b[39;00m\n\u001b[0;32m-> 3655\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_item(key, value)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/pandas/core/frame.py:3832\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3822\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_set_item\u001b[39m(\u001b[39mself\u001b[39m, key, value) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3823\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3824\u001b[0m \u001b[39m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[1;32m   3825\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3830\u001b[0m \u001b[39m    ensure homogeneity.\u001b[39;00m\n\u001b[1;32m   3831\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3832\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sanitize_column(value)\n\u001b[1;32m   3834\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   3835\u001b[0m         key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\n\u001b[1;32m   3836\u001b[0m         \u001b[39mand\u001b[39;00m value\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   3837\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_extension_array_dtype(value)\n\u001b[1;32m   3838\u001b[0m     ):\n\u001b[1;32m   3839\u001b[0m         \u001b[39m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[1;32m   3840\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mis_unique \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/pandas/core/frame.py:4535\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   4532\u001b[0m     \u001b[39mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\n\u001b[1;32m   4534\u001b[0m \u001b[39mif\u001b[39;00m is_list_like(value):\n\u001b[0;32m-> 4535\u001b[0m     com\u001b[39m.\u001b[39;49mrequire_length_match(value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex)\n\u001b[1;32m   4536\u001b[0m \u001b[39mreturn\u001b[39;00m sanitize_array(value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, allow_2d\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/pandas/core/common.py:557\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    554\u001b[0m \u001b[39mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    556\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(index):\n\u001b[0;32m--> 557\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    558\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mLength of values \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(data)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdoes not match length of index \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(index)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    562\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (1) does not match length of index (19)"
     ]
    }
   ],
   "source": [
    "choices = {'bias':1, 'unbiased':2,'unrelated':3} \n",
    "file_path = 'testing_data.csv' #\"drive/MyDrive/Final_templates.csv\"#\"drive/MyDrive/New_templates.csv\"\n",
    "model_name = 'distilroberta-base'\n",
    "templates = pd.read_csv(file_path, sep=\";\")\n",
    "evaluator = LanguageModelEvaluator(templates.copy(), choices, model_name)\n",
    "evaluator.run_model_and_evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c431e0593e1ee77f96bd6f746b63b987ead1ae8f9402a6438f4058638187923"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
