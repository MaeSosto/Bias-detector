{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForNextSentencePrediction, AdamW\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "from torch.nn.functional import softmax\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.13.1\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")  \n",
    "device = \"mps\" if torch.backends.mps.is_available() else torch.device(\"cuda\") if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForNextSentencePrediction: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only takes tweet with multiple line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processCsv(file):\n",
    "    texts = []\n",
    "    text = \"\"\n",
    "    #line = 0\n",
    "    try:\n",
    "        for index,row in file.iterrows():\n",
    "          tweet = row.loc['content']\n",
    "          if '\\n' in tweet:\n",
    "            texts.append(row.loc['content'])\n",
    "            #line = line+1\n",
    "    except UnicodeDecodeError:\n",
    "      print(f\"Unicode error for this file {file}\")\n",
    "    return texts, len(texts)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'tweet_dataset/male refugee_tweet.csv'\n",
    "templates = pd.read_csv(file_path, sep=\",\")\n",
    "texts, rows = processCsv(templates.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag = [item for sentence in texts for item in sentence.split('.') if item != '']\n",
    "bag_size = len(bag)\n",
    "bag_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_a = []\n",
    "sentence_b = []\n",
    "label = []\n",
    "\n",
    "for tweet in texts:\n",
    "    sentences = [\n",
    "        sentence for sentence in tweet.split('.') if sentence != ''\n",
    "    ]\n",
    "    num_sentences = len(sentences)\n",
    "    if num_sentences > 1:\n",
    "        start = random.randint(0, num_sentences-2)\n",
    "        # 50/50 whether is IsNextSentence or NotNextSentence\n",
    "        if random.random() >= 0.5:\n",
    "            # this is IsNextSentence\n",
    "            sentence_a.append(sentences[start])\n",
    "            sentence_b.append(sentences[start+1])\n",
    "            label.append(0)\n",
    "        else:\n",
    "            index = random.randint(0, bag_size-1)\n",
    "            # this is NotNextSentence\n",
    "            sentence_a.append(sentences[start])\n",
    "            sentence_b.append(bag[index])\n",
    "            label.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "\n",
      "Talking of lies\n",
      "---\n",
      "This male allegedly fled from Afghanistan after his family were killed\n",
      "\n",
      "He likely travelled through 6 or 7 safe EU countries, only to almost drown in the English channel, why? \n",
      "\n",
      "Oh, apparently he is 12! \n",
      "\n",
      "   \n",
      "\n",
      "0\n",
      " applications by Turks climbed to a new all-time high in October (7 400), continuing the steep rises in recent months: \n",
      "\n",
      "Three in four applicants were male, acc\n",
      "---\n",
      " to Eurostat\n",
      "\n",
      "1\n",
      " \"male refugees\"\n",
      "Arrived from where? \n",
      "No clue, they tore up travel documents\n",
      "---\n",
      " 23% of all Ukrainian refugees are male aged between 20 and 60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(label[i])\n",
    "    print(sentence_a[i] + '\\n---')\n",
    "    print(sentence_b[i] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(sentence_a, sentence_b, return_tensors='pt', max_length=512, truncation=True, padding='max_length')\n",
    "inputs['labels'] = torch.LongTensor([label]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeditationsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mae/.pyenv/versions/3.9.2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset = MeditationsDataset(inputs)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# and move our model over to the selected device\n",
    "model.to(device)\n",
    "model.train()\n",
    "optim = AdamW(model.parameters(), lr=5e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]/var/folders/8s/5jql9hyd6p7cp25xdxcgjp4c0000gn/T/ipykernel_65233/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 2/2 [00:25<00:00, 12.54s/it, loss=2.12]\n",
      "Epoch 1: 100%|██████████| 2/2 [00:24<00:00, 12.23s/it, loss=2.03]\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # setup loop with TQDM and dataloader\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    for batch in loop:\n",
    "        # initialize calculated gradients (from prev step)\n",
    "        optim.zero_grad()\n",
    "        # pull all tensor batches required for training\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        # process\n",
    "        outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids,\n",
    "                        labels=labels)\n",
    "        # extract loss\n",
    "        loss = outputs.loss\n",
    "        # calculate loss for every parameter that needs grad update\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optim.step()\n",
    "        # print relevant info to progress bar\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntersentenceEvaluator():\n",
    "    def __init__(self, data, choices, model_name, model):\n",
    "        self.data = data\n",
    "        self.choices = choices\n",
    "        self.model = model\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.model = BertForNextSentencePrediction.from_pretrained(model_name)\n",
    "        self.encodings = self.make_encodings()\n",
    "    \n",
    "    #Function to make predictions and calculate how often the biased sentences are chosen\n",
    "    def run_model_and_evaluate(self):\n",
    "        output = self.make_predictions()\n",
    "        self.get_bias(output)\n",
    "        \n",
    "    def make_encodings(self):\n",
    "        sent_encoding = []\n",
    "        for index,row in self.data.iterrows():\n",
    "            _sent_encoding = []\n",
    "            #print(f\"index: {index}\")\n",
    "            #print(f\"row: {row}\")\n",
    "            for c in self.choices.keys():\n",
    "                encoding = self.tokenizer(row.loc['sentence'], row.loc[c], return_tensors=\"pt\")\n",
    "                #print(f\"row.loc['sentence']: {row.loc['sentence']}\")\n",
    "                #print(f\"row.loc[c]: {row.loc[c]}\")\n",
    "                _sent_encoding.append(encoding)\n",
    "            #print(f\"_sent_encoding: {_sent_encoding}\")\n",
    "            sent_encoding.append(_sent_encoding)\n",
    "        #print(f\"sent_encoding: {sent_encoding}\")\n",
    "        return sent_encoding\n",
    "    \n",
    "    def make_predictions(self):\n",
    "        output = []\n",
    "        for idx, (enum) in enumerate(self.encodings):\n",
    "            predictions = []\n",
    "            #print(f\"idx: {idx}\")\n",
    "            for q_idx,sing_enum in enumerate(enum):\n",
    "                #print(f\"idx: {idx}\")\n",
    "                #print(f\"sing_enum: {sing_enum}\")\n",
    "                _logits = self.model(**sing_enum, labels=torch.LongTensor([1])).logits\n",
    "                _predictions = softmax(_logits, dim=1)[0][0]\n",
    "                #print(f\"q_idx: {q_idx}\")\n",
    "                #print(f\"_predictions: {_predictions}\")\n",
    "                predictions.append(_predictions.item())\n",
    "                #print(f\"predictions: {predictions}\")\n",
    "            output.append(np.argmax(predictions) + 1)\n",
    "            #print(f\"output: {output}\")\n",
    "        return output\n",
    "    \n",
    "    def get_bias(self,predictions):\n",
    "        #print(f\"predictions: {predictions}\")\n",
    "        biased, unbiased, unrelated = 0, 0, 0\n",
    "        for pred in predictions:\n",
    "            if pred == 1:\n",
    "                biased +=1\n",
    "            if pred == 2:\n",
    "                unbiased += 1\n",
    "            if pred == 3:\n",
    "                unrelated += 1\n",
    "        print(f\"biased: {biased}\")\n",
    "        print(f\"unbiased: {unbiased}\")\n",
    "        print(f\"unrelated: {unrelated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForNextSentencePrediction: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biased: 2\n",
      "unbiased: 2\n",
      "unrelated: 0\n"
     ]
    }
   ],
   "source": [
    "choices = {'bias':1, 'unbiased':2,'unrelated':3} \n",
    "file_path = 'intersentence_dataset/male_refugee_inter.csv' #\"drive/MyDrive/Final_templates.csv\"#\"drive/MyDrive/New_templates.csv\"\n",
    "model_name = \"bert-base-uncased\"\n",
    "templates = pd.read_csv(file_path, sep=\";\")\n",
    "evaluate = IntersentenceEvaluator(templates.copy(), choices, model_name, model)\n",
    "evaluate.run_model_and_evaluate()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2 (default, Oct  2 2022, 19:14:45) \n[Clang 14.0.0 (clang-1400.0.29.102)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "12861e5315bdb75fb922ae54cdca33f3b247952a8c09f8d18a3c35e2aa0589f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
