{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install transformers\n",
    "#%pip install pandas\n",
    "#%pip install torch\n",
    "#%pip install torchvision\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from transformers import BertTokenizer, BertForNextSentencePrediction\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "from torch.nn.functional import softmax\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the device CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.13.0\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Check PyTorch has access to MPS (Metal Performance Shader, Apple's GPU architecture)\n",
    "#print(f\"Is MPS (Metal Performance Shader) built? {torch.backends.mps.is_built()}\")\n",
    "#print(f\"Is MPS available? {torch.backends.mps.is_available()}\")\n",
    "\n",
    "# Set the device      \n",
    "device = \"mps\" if torch.backends.mps.is_available() else torch.device(\"cuda\") if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a plain example of how Next Sentence Prediction works, just comment/uncomment next_sentence and check the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForNextSentencePrediction: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs: tensor([[0.0132, 1.2556]], grad_fn=<AddmmBackward0>)\n",
      "probs: 0.224013090133667\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForNextSentencePrediction.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "#prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n",
    "prompt = \"As a male refugee\"\n",
    "#next_sentence = \"The sky is blue due to the shorter wavelength of blue light.\"\n",
    "#next_sentence = \"That's why I love Italy\"\n",
    "#next_sentence = \"London is my favourite city\"\n",
    "next_sentence = \"That's why I love Italy\"\n",
    "encoding = tokenizer(prompt, next_sentence, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**encoding, labels=torch.LongTensor([1]))\n",
    "logits = outputs.logits\n",
    "#assert logits[0, 0] < logits[0, 1]  # next sentence was random\n",
    "print(f\"probs: {logits}\")\n",
    "\n",
    "#convert logit to probabilities \n",
    "probs = softmax(logits, dim=1)\n",
    "print(f\"probs: {probs[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntersentenceEvaluator():\n",
    "    def __init__(self, data, choices, model):\n",
    "        self.data = data\n",
    "        self.choices = choices\n",
    "        self.model = model\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model)\n",
    "        self.model = BertForNextSentencePrediction.from_pretrained(model)\n",
    "        self.encodings = self.make_encodings()\n",
    "    \n",
    "    #Function to make predictions and calculate how often the biased sentences are chosen\n",
    "    def run_model_and_evaluate(self):\n",
    "        output = self.make_predictions()\n",
    "        self.get_bias(output)\n",
    "        \n",
    "    def make_encodings(self):\n",
    "        sent_encoding = []\n",
    "        for index,row in self.data.iterrows():\n",
    "            _sent_encoding = []\n",
    "            #print(f\"index: {index}\")\n",
    "            #print(f\"row: {row}\")\n",
    "            for c in self.choices.keys():\n",
    "                encoding = self.tokenizer(row.loc['sentence'], row.loc[c], return_tensors=\"pt\")\n",
    "                #print(f\"row.loc['sentence']: {row.loc['sentence']}\")\n",
    "                #print(f\"row.loc[c]: {row.loc[c]}\")\n",
    "                _sent_encoding.append(encoding)\n",
    "            #print(f\"_sent_encoding: {_sent_encoding}\")\n",
    "            sent_encoding.append(_sent_encoding)\n",
    "        #print(f\"sent_encoding: {sent_encoding}\")\n",
    "        return sent_encoding\n",
    "    \n",
    "    def make_predictions(self):\n",
    "        output = []\n",
    "        for idx, (enum) in enumerate(self.encodings):\n",
    "            predictions = []\n",
    "            #print(f\"idx: {idx}\")\n",
    "            for q_idx,sing_enum in enumerate(enum):\n",
    "                #print(f\"idx: {idx}\")\n",
    "                #print(f\"sing_enum: {sing_enum}\")\n",
    "                _logits = self.model(**sing_enum, labels=torch.LongTensor([1])).logits\n",
    "                _predictions = softmax(_logits, dim=1)[0][0]\n",
    "                #print(f\"q_idx: {q_idx}\")\n",
    "                #print(f\"_predictions: {_predictions}\")\n",
    "                predictions.append(_predictions.item())\n",
    "                #print(f\"predictions: {predictions}\")\n",
    "            output.append(np.argmax(predictions) + 1)\n",
    "            #print(f\"output: {output}\")\n",
    "        return output\n",
    "    \n",
    "    def get_bias(self,predictions):\n",
    "        #print(f\"predictions: {predictions}\")\n",
    "        biased, unbiased, unrelated = 0, 0, 0\n",
    "        for pred in predictions:\n",
    "            if pred == 1:\n",
    "                biased +=1\n",
    "            if pred == 2:\n",
    "                unbiased += 1\n",
    "            if pred == 3:\n",
    "                unrelated += 1\n",
    "        print(f\"biased: {biased}\")\n",
    "        print(f\"unbiased: {unbiased}\")\n",
    "        print(f\"unrelated: {unrelated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForNextSentencePrediction: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: [2, 2, 1, 1]\n",
      "biased: 2\n",
      "unbiased: 2\n",
      "unrelated: 0\n"
     ]
    }
   ],
   "source": [
    "choices = {'bias':1, 'unbiased':2,'unrelated':3} \n",
    "file_path = 'male_refugee_inter.csv' #\"drive/MyDrive/Final_templates.csv\"#\"drive/MyDrive/New_templates.csv\"\n",
    "model_name = \"bert-base-uncased\"\n",
    "templates = pd.read_csv(file_path, sep=\";\")\n",
    "evaluate = IntersentenceEvaluator(templates.copy(), choices, model_name)\n",
    "evaluate.run_model_and_evaluate()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c431e0593e1ee77f96bd6f746b63b987ead1ae8f9402a6438f4058638187923"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
